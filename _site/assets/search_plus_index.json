{"/pages/Activity08/": {
    "title": "Activity: Sorting as Pre-Processing",
    "keywords": "Activity Sorting Design",
    "url": "/pages/Activity08/",
    "body": "Learning Goals You will work towards being able to… Understand sorting as a pre-processing step for solving common algorithmic problems. Turn algorithmic ideas into pseudocode Instructions Work with your groups on the following problems: Sorting for Search: As Skiena tells us, preprocessing an array with a sorting algorithm lets us use Binary Search instead of Linear Search to find some element. If we only search the array once, is this a good strategy? How many times must we search in order for preprocessing with an $\\Theta(n \\log n)$ sort to be worth it? Make an argument in terms of Big-Oh time complexities. Note that this is a form of amortized analysis! Good-Enough Subset Sum: Given an array $A$ of Integers and an Integer $k$, find a set $S$ that contains the maximum number of elements in $A$ that sum to $\\leq k$. For example, If you were given $A = [4, 2, 8, -1, 7]$ and $k = 3$, we should find either $S = {2, -1}$ or $S = {4, -1}$. Provide pseudocode for a solution and provide an informal analysis of it’s time complexity. Maximum Gap: Given an array $A$ of Integers, find the pair of Integers $a, b \\in A$, $a &lt; b$ such that (1) there is no $c \\in A$ such that $a &lt; c &lt; b$ and (2) $\\lvert a - b \\rvert$ is maximized. That is, find an element $a$ such that the next largest element in $A$, $b$, is as far away as possible. For example, given $A = [4, 2, 1, 14, -7]$, you should find the pair $(4, 10)$. Again, provide pseudocode and an informal analysis of it’s time complexity. 1 k-Tuple Sum: Given an array $A$ of $n$ integers and an integer $t$, find an $O(n^{k-1}\\log n)$ algorithm to test whether $k$ of the integers in $S$ add up to $t$ (Skiena 4-9). Submission Submit an artifact of your work from any of 1–4 on Moodle. This is a variation on LeetCode problem 164! They ask you to do this in Linear time &amp; space, which isn’t doable with $\\Theta(n \\log n)$ sort preprocessing, unfortunately. However, the idea is still to preprocess with sorting! &#8617;"
  },"/pages/ActivityBS/": {
    "title": "Activity: Binary Search Variants",
    "keywords": "Activity Sorting Design",
    "url": "/pages/ActivityBS/",
    "body": "Learning Goals You will work towards being able to… Understanding the core idea of common algorithms (BinarySearch) Turn algorithmic ideas into pseudocode Apply Divide-and-Conquer design techniques to novel problems Instructions Work with your groups on the following problems: Approximately how wrong: Let’s suppose we want to find a the decimal expansion of a root of a polynomial $f(n)$ to a certain precision. While Skiena tells us that each iteration of binary search gets us closer and closer to the true root, how much closer does each iteration get us? Suppose we have some polynomial $f(x)$ that is guaranteed to have some root $x$ that is between a given $l$ and $r$ (i.e., $\\exists r$, $l \\leq x \\leq r$ such that $f(x) = 0$). Suppose we also have some notion of the precision we desire, $\\varepsilon &gt; 0$, and we want to find some $x’$ such that $x - \\varepsilon \\leq x’ \\leq x + \\varepsilon$. Given all this, how many iterations of the bisection method do we need to call before we can guarantee we can pick such a $x’$? I recommend you first step through the algorithm a few times to get a feel for it’s behavior, and then sketch out a concrete solution for a particular set of choices. Then try and generalize. If you’re confident with that, prove this to be true by constructing a loop invariant! With great power comes great responsibility (with computing resources): Suppose you are a statistician consulting for psychologist. They are worried about statistical power for their upcoming experiment (i.e., they’re concerned that they won’t collect enough data from enough participants to make valid statistical inference). They’ve determined that if they don’t have a statistical power $&gt;0.8$, it’s not worth running the experiment. However, they don’t want to run too many participants, since they have to pay each one, and money is tight. Luckily, you know how to do a power analysis, a computationally intense algorithm that can determine the statistical power given a particular number of subjects. Call the number of subjects $n$. Luckily, you know one helpful fact: the more participants you run, the higher the power you have! What strategy would you use to guarantee the experiment has $&gt;.80$ power, minimize the cost of recruiting participants, and minimize the amount of computational power spent (i.e., minimize the number of power analyses run?).1 Submission Submit an artifact of your work from either 1 or 2. As you might have guessed from the detailed framing and my research area, I did this in some work I did a few years ago, though I was playing both roles. &#8617;"
  },"/pages/ActivityDP1/": {
    "title": "Activity: Dynamic Programming Practice",
    "keywords": "Activity dynamic programming dp",
    "url": "/pages/ActivityDP1/",
    "body": "Learning Goals You will work towards being able to… Design dynamic programming solutions to algorithmic problems Activity For each of the following questions, (1) design a recurrence relation, (2) provide pseudocode for a naive D&amp;C solution, and finally (3) develop a dynamic programming algorithm to solve it. Consider the time and space complexity of your solutions. It WILL be helpful to play around with the problem specification to build intution (i.e., work through an example!) before jumping to designing an algorithm. (Skiena 10-1) A child running up a staircase can hop between 1 and $k$ steps at a time. Suppose the staircase is $n$ steps tall. How many unique ways can the child climb the staircase? If you prefer a more math-y framing, how many unique sequences of integers $s_1, s_2, \\dots, s_l$, where $\\forall 1 \\leq i \\leq l, 1 \\leq s_i \\leq k$ exist such that $\\sum s_i = n$? (Skiena 10-5) Consider an $s$ x $t$ grid $G$ filled with non-negative numbers. Find a “path” from the top left to bottom right corner than minimizes the sum of numbers along the path. That is, you start at the top-left corner, and at each step can choose to move down within the same column or right within the same row. Submission Submit an artifact of your work on Moodle. You need not complete every (or even any) part, but you are responsible for eventually understanding how these problems work!"
  },"/pages/ActivityDP2/": {
    "title": "Activity: Dynamic Programming Practice 2",
    "keywords": "Activity dynamic programming dp",
    "url": "/pages/ActivityDP2/",
    "body": "Learning Goals You will work towards being able to… Design dynamic programming solutions to algorithmic problems Activity For each of the following questions, design a recurrence relation, and then provide pseudocode for a dynamic programming algorithm to solve it. Consider the time and space complexity of your solutions. It may be helpful to play around with the problem specification to build intution (i.e., work through an example!) before jumping to designing an algorithm. (Skiena 10-6) Modify the edit distance algorithm we’ve discussed to allow for transposition errors, where at the cost of one operation, you can swap two adjacent characters. Convince yourself that your recurrence works, and run your pseudocode on the “steve”/”setve” pair Skiena gives you. (Skiena 10-7) Suppose you have 3 strings, $x, y, z$, where $\\lvert z \\rvert = \\lvert x \\rvert + \\lvert y \\rvert$. $z$ is a shuffle of $x$ and $y$ if it interleaves characters from $x$ and $y$ preserving their order. That is, you take $x$ and insert all characters of $y$ into the string such that the order of characters in $y$ are preserved. See Skiena 10-7a for examples — confirm that they make sense! Then write a dynamic programming algorithm (based on the structure of edit distance!) that determines whether $z$ is a shuffle of $x$ and $y$. Submission Submit an artifact of your work on Moodle."
  },"/pages/ActivityDS/": {
    "title": "Activity: Data Structure Review",
    "keywords": "Activity DataStructures Review",
    "url": "/pages/ActivityDS/",
    "body": "Learning Goals You will work towards being able to… Know the time complexities of common implementations of hashtables, priority queues, trees, and graphs Reason about best- and worst-case inputs to these data structures Use the vocabulary of non-linear data structures: hashing/hashcode, loading factor, parent, child, sister/sibling, ancestor/descendent, root, directed/undirected, dense/sparse, connected/connected component, cycle/cyclic/acyclic. Over and above this, the general goal of this assignment is to start getting you to think about these data structures as abstract mathematical objects rather than constructs in programming. Instructions Work with your groups on the problems below. Each section also includes the question marked Bonus, which you should only attempt AFTER solving all of the non-bonus questions in each section. I encourage you to work through these problems with examples. Some of these questions are fairly abstract, and so it may be tempting to work through them entirely abstractly. However, you might find that your intuitions about these abstract structures aren’t great (or worse, wrong!). I recommend that you develop explicit, concrete examples for each of these problems. This may involve working through the problem after picking specific values for unknown constrants (i.e., solve the hashing question with $m=5$ and then with $m=6$! How does changing $m$ change how collisions work out? Can I generalize to all possible $m$?), or explicitly writing out the adjacency list and matrix representations for a few graphs before you get an intuition for which ones are best suited for adjacency matrices vs. lists. Hashing and Maps Suppose you have a hashtable of size $m$ that you use to implement a map with String keys and Integer values. You map keys to indices in the hashtable using the following function: function hash(String s, Integer m): hc &lt;- 0 for i &lt;- 1 to length(s): hc &lt;- hc + letter_num(s[i]) return hc % m Where letter_num returns the position of the letter in the alphabet (i.e., letter_num(“a”) = 1, letter_num(“b”) = 2, …, letter_num(“z”) = 26). For what values of $m$ will the following insertions collide? insert(\"bald\") insert(\"area\") ~~insert(\"sea\")~~ What about the following inserts? insert(\"cheddar\") insert(\"comic\") insert(\"fern\") insert(\"nerf\") Can you conclude anything in-general about your choice of $m$ if you were to implement this hashmap? what $m$ would minimize collisions? What collisions can you not avoid? (If it helps, feel free to make some assumptions: What if the length of string keys have length $\\leq k$, for some $k$? Can you construct a good $m$ in terms of $k$). Feel free to write small pieces of code if it helps you work through the problem! Bonus: For those of you with some probability background, let’s consider the odds of a collision occurring: Suppose you have a hash function $f$ that is distributed uniformly. This means for random $k$ in our space of keys, the corresponding values $f(k)$ are equally likely to be hashed to any of the $m$ slots in our hashtable. Formally, we can say that this means for a sequence of insertions with keys $k_1, \\dots, k_n$, $P(f(k_i) = j) = \\frac{1}{m}$ for any $1 \\leq i \\leq n$ and $1 \\leq j \\leq m$. What is the likelihood of a collision after $n$ insertions (that is, that $\\exists l,m$ such that $f(k_l) = f(k_m)$). Like many probability problems of this type, it might be easier to reason about the likelihood that there aren’t any collisions. PQueues and Heaps Suppose you have a priority queue implemented with a min-heap. Suppose 9 items were successfully inserted into it. What is the structure of the tree? Draw the nodes and parent-child structure. HINT How does a heap maintain balance? What property does the tree underlying the heap always have? Recall that a min-heap has an ordering property such that if node $a$ is a child of node $b$, then $a \\geq b$. Show (formally) that this means that a path from root to leaf $c_1, c_2, \\dots, c_h$ is sorted. For clarity, since $c_1, \\dots c_h$ is a path from root to leaf, this means that for each $1 &lt; i \\leq h$, $c_i$ is the child of $c_{i-1}$. Bonus: When we insert or remove an element from the heap, our algorithm from data structures was to insert it to an incorrect position (i.e., one that violates our ordering property), and then swap it with elements above until it was greater than it’s parent. This is, essentially, a single pass of BubbleSort on the path from the root to the new leaf. Show that this always works! Make sure to use the proper assumptions: What do you know about the structure of the heap before the insert? Write out the pseudocode for the insert and see if you can prove it’s correctness (i.e., that the element is added and that the heap ordering and balance properties are preserved. (Binary Search) Trees Suppose you have an empty (non-self-balancing) binary search tree and insert $k$ elements into it. What is the worst-case time complexity of a look-up for a particular value (in big-$\\Theta$). What does a worst case sequence of insertions look like? Bonus (Skiena 3-15) Suppose you want to convert an un-balanced binary search tree to a balanced one. Write an $O(n)$ algorithm that will convert a binary search tree with $n$ nodes to one that is perfectly balanced. Here, take balanced to mean that the depth of any two null pointers (i.e., a missing child) differs by at most 1. &gt; ##### HINT &gt; Look back at the answer to the hint question in the PQueue/Heap section. Could you borrow a balancing idea from there? {: .block-tip} BONUS: (Skiena 3-21) Given two BSTs $A$ and $B$ such that $\\forall a \\in A, b \\in B$, $a &lt; b$, write pseudocode to construct a new BST $C$ that contains all of the elements in $A$ and $B$ (that is, $\\forall a \\in A$, $a \\in C$ and $\\forall b \\in B$, $b \\in C$). Do this in $O(h)$ (worst-case) time, where $h$ is the maximum height between $A$ and $B$. &gt; ##### HINT &gt; Focus on ensuring that $C$ satisfies the ordering property for BSTs. Both trees are given to you as ordered BSTs, so try to change the structure of $A$ and $B$ as little as possible when merging them. {: .block-tip} Graphs Construct a graph which will take up less memory as an adjacency list vs. as an adjacency matrix. Consider a complete graph $G = (V, E)$. Being complete means that for any $v_1, v_2 \\in V$, $(v_1, v_2) \\in E$ (every pair of vertices has an edge connecting them). Suppose the graph in undirected. What is $|E|$? That is, how many edges are in $G$? For simplicity, allow for self-edges. What is the smallest $|E|$ one can have such that $G = (V, E)$ is connected? BONUS: We can think of trees as a special case of graphs. Two equivalent definitions are (1) an undirected graph that is connected and acyclic or (2) a graph where any two vertices are connected by exactly 1 path. Convince yourself (and attempt to show, formally) that (1) these definitions match the way we talk about trees and (2) that these two definitions are equivalent. Submission Upload to Moodle your work on at least one problem per data structure. Convince yourself that you could solve all the problems here independently (with the exception of the bonus problems) to make sure that you’re sufficiently caught up with data structures!"
  },"/pages/ActivityHS/": {
    "title": "Activity: Heap- &amp; TreeSort",
    "keywords": "Activity DataStructures Sorting TreeSort HeapSort",
    "url": "/pages/ActivityHS/",
    "body": "Learning Goals You will work towards being able to… Build and reason about algorithms that use data structure-based design principles Understanding the core idea of, write pseudocode for, and analyze the runtime and correctness of common algorithms (HeapSort and TreeSort) Understand sorting as a pre-processing step for solving common algorithmic problems. Part 1: HeapSort Consider the following pseudocode for SelectionSort. Make sure you understand how it works (HW2 should help!). SelectionSort(Array A[1...n]): for i ← 1 to n-1: idx ← (i-1) + Select(A[i...n]) // Gets the index of the smallest element in the subarray Swap(A[idx], A[i]) return A Note that the implementation of Select you saw in the homework operates by performing a linear scan of the subarray, as follows: Select(Array A[1...m]): min_idx ← 1 for i ← 2 to m: if A[i] &lt; A[min_idx]: min_idx ← i return i Make sure you understand what this code is doing! The goal of Data Structure-based Design is to identify code that navigates data (i.e., a collection) inefficiently and modifying the code to replace these inefficiencies with code that uses a better data structure that is faster and still correct. What is the Big-O worst-case time complexity of select? We define correctness for select as returning the index of the smallest element in the subarray passed as an argument. Recall from data structures that we spend some time designing a data structure that made repeatedly finding the smallest element in a collection fast and efficient! Spend a moment reminding yourself and your partner of this data structure (a priority queue, implemented via a min-heap!). What are the time complexities of the add and removeMin operations in a min-heap/pqueue? Rewrite SelectionSort so that it utilizes a priority queue rather than the Select function. Analyze the time complexity of the resulting HeapSort function. You may use the fact that $\\sum_{i=1}^{n} \\log i \\in \\Theta(n\\log n)$ (a consequence of Stirling’s approximation, if you’re interested). Bonus: Just as you prove the correctness of SelectionSort using an intermediate results/lemma about Select being correct, we prove the correctness of heapsort using established results about the correctness of our heap operations! For example, we can modify our proof of correctness for SelectionSort to be one for HeapSort by using a few facts about heaps we can prove. Namely, we can say state results like (Lemma 1) For a min-heap $h$, removeMin() updates $h$ to $h’$ returns an element $e$ such that $e \\leq f$ for all $f \\in h’$, $e \\notin h’$, and for all $f \\in h$ where $e \\neq f$, $f \\in h$. which simply rephrases that removeMin returns the smallest element of the heap and removes it (and only it) from the heap. (a.) Think about how you might adapt your proof of correctness from HW2 to prove HeapSort correct. (b.) Now consider the implementation side of the proof. Consider how we might write a proof for the above lemma using our implementation of a heap from 128! Part 2: TreeSort Write out pseudocode for TreeSort, which works by inserting each element of our array into a Binary Search Tree and filling in a new array by reading out the search tree’s in-order traversal. This should notably be more specified than the problem in HW0! Work backwards from TreeSort and compare it’s behavior the the $O(n^2)$ sorts we’ve seen before. Discuss with your partner how TreeSort can be analyzed as a Data Structure-based optimization of a sort we’ve seen before. Recall what we’ve seen from discussing Self-Balancing BSTs and provide a Big-O time complexity for Insert into a SBBST. We will have to wait briefly for a formal analysis of an in-order traversal, but you may assume what we said in Data Structures — that it is $O(n)$. What is the time complexity of TreeSort? Bonus: Lets begin analyze TreeSort’s correctness (and runtime, if we didn’t discuss this in lecture!). You’ll find (similar to HeapSort) that the correctness of this algorithm relies on a property of a binary search tree: That it’s in-order traversal is sorted! Use the following definitions: A sequence $a_1, \\dots a_n$ is sorted iff $a_1 \\leq a_2 \\leq \\dots \\leq a_n$. A sequence $a_1, \\dots, a_n$ is an in-order traversal of a binary tree $T$ iff each $a_i$, $1 \\leq i \\leq n$ uniquely maps to the value of a node in $T$ and for any $a_i$ and it’s corresponding node $N$, the value of each node in the left subtree of $N$, $a_j$ has index $j &lt; i$ and every element in it’s right subtree of $N$, $a_k$, has index $k &gt; i$. A binary search tree is a tree $T$ such that for every node $N$ with value $m$ in the tree, every value $l$ in the left subtree of $N$ has $l \\leq m$ and for every value $r$ in the right subtree of $N$ has $r \\geq m$. Submission Submit an artifact of your work from the HeapSort portion of this assignment, as well as your TreeSort pseudocode if you’ve gotten there!"
  },"/pages/ActivityLoopInv/": {
    "title": "Activity: Sorting and Loop Invariants",
    "keywords": "Activity Sorting",
    "url": "/pages/ActivityLoopInv/",
    "body": "Learning Goals You will work towards being able to… Prove the correctness of an iterative algorithm using a loop invariant Warm-Up For each of the following iterative functions, write a loop invariant that you can use to prove it correct. If you have extra time, sketch out a proof. We define the factorial of $n \\in \\mathbb{N}$ (i.e., $n!$) as $n! = \\prod_{i=1}^n i$. Consider: Factorial(n ∈ ℕ): p ← 1 for i ← 1 to n: p ← p * i return p We define the Fibonacci sequence $f_0, f_1, \\dots $ as follows: $f_0 = 0$, $f_1 = 1$, and for $n \\geq 2$, $f_n = f_{n-1} + f_{n-2}$. A solution to the Fibonacci Number problem takes in $n \\in \\mathbb{N}$ and must return $f_n$ as defined. Consider the following: Fibonacci(n ∈ ℕ): f_prev ← 0 f ← 1 for i ← 2 to n: f ← f + f_prev f_prev ← f - f_prev return f Activity Instructions Consider the following BubbleSort algorithm. Read through the code, verify you understand the steps, and simulate the algorithm with the cards in front of you. BubbleSort(Array A[1...n]): for i ← 1 to n for j ← 1 to n-i: if A[j+1] &lt; A[j]: swap(A[j], A[j+1]) return A As your simulate the code, begin to think about what each loop in the code does. Then, as you perform each loop, keep in mind our core questions: How does each iteration make progress toward our ultimate goal?. Make sure your group has a solid ointuition for this before moving on. It may be helpful to grab a marker and keep track of the state of the array after each iteration. Look for patterns that you might be able to describe as invariants! Split the code into two functions (like Insert and InsertionSort) that isolate each loop. Call the new function that represents the inner loop Bubble, to fit the theme. Write a problem definition for the problem each function solves using your idea of what each loop is supposed to do. This should be simple for the outer loop, since the algorithm needs to sort! The inner loop will be more interesting. Identify a loop invariant for the loop in each function. Sketch out a proof (i.e., don’t worry about phrasing for now, just the ideas!) for each invariant, starting with the Bubble function and using that result to prove the outer Loop Invariant. Don’t be worried if you feel like you don’t have enough to show your loop invariant holds! Think about what additional information might be missing and how you might strengthen your loop invariant to give you a stronger inductive hypothesis!"
  },"/pages/ActivityMT/": {
    "title": "Activity: The Master Theorem",
    "keywords": "Activity Sorting Design Divide and Conquer",
    "url": "/pages/ActivityMT/",
    "body": "Learning Goals You will work towards being able to… State the definition of the Master Theorem Determine the runtime complexity of recursive functions Use the Master Theorem to understand when a divide and conquer approach results in a faster algorithm Instructions For each of the following growth functions, solve the recurrence informally, unrolling the recurrence or drawing the recurrence tree (described below) and determine which terms will dominate the time complexity. Then compare your intuitions to the result you get by applying the Master Theorem. To draw a recurrence tree, you represent each call with a node, where the node is labeled with the time complexity done for that call at that recursive depth and has children that correspond to each recursive call made when executing that function call. This is useful because then we can think of the total work done by the algorithm as the sum of all of the work done at each node. Observe that since leaves have no children, they represent calls that enter the base case! For example, for MergeSort, our structure would look something like the below image, which should look like what you saw in lecture. Each function represents the time complexity of a call to Merge, which dominates the non-recursive portion of each call to MergeSort, and that node’s children represent the recursive calls made from that initial call. Note that we work out the height of our tree (how many recursions until our problem size is 1?) and the number of leaves in our tree (after all of our recursions, how many branches do we create?). Then observe that the sum of all of these runtimes is our total runtime! As you draw and analyze each tree, pay attention to which terms will dominate the total runtime. Then compare your findings to the master theorem $T(n) = 2T(\\frac{n}{3}) + n^2$ $T(n) = 3T(\\frac{n}{2}) + n$ $T(n) = 4T(\\frac{n}{2}) + n^2$ Submission Submit an artifact of your work from either 1 or 2 or 3."
  },"/pages/ActivityQS/": {
    "title": "Activity: Quicksort and Partitioning",
    "keywords": "Activity Sorting Quicksort Partitioning",
    "url": "/pages/ActivityQS/",
    "body": "Learning Goals You will work towards being able to… Understanding the behavior of common algorithms (quicksort) Implement common algorithms (quicksort) Warm-Up As a group, walk through a few examples of the partitioning algorithm you saw in the reading. Construct a few random arrays and step through the code line-by-line to understand how the array is structured and maintained throughout. With an array of fixed size $n$ and fixed elements, which permutation of the array is best? Which is worst? Instructions Work with your groups on the following problems: Based on our sketch of the proof of correctness for quicksort in lecture, define correctness for the partition algorithm and construct a proof of correctness for the partition algorithm. Feel free to sketch things out (Find the right loop invariant, etc.) while you’re with your group and fill out details on your own. Bonus: This partitioning algorithm isn’t the only (or most efficient, or even first) partitioning algorithm for quicksort. However, this version (called the Lomuto scheme) is generally regarded as a simpler, easier to implement version that doesn’t sacrifice asymptotic time complexity. The other common scheme, the Hoare scheme, is faster, but more complex. It’s also clever in a very intro algorithms sort of way, so it may be worth seeing the trick! partition(A, low, high): pivot &lt;- A[low] i &lt;- low j &lt;- high While True while A[i] &lt; pivot: i &lt;- i + 1 while A[j] &gt; pivot: j &lt;- j - 1 if j &lt;= i: Return j Swap(A[i], A[j]) i &lt;- i + 1 j &lt;- j - 1 Here the idea is to move two indices from the edges toward the center until we find an element on the left that should be on the right and an element on the right that should be on the left. If that happens, we swap them! When the two indices cross, we swap them. Work through a few array with this partition algorithm until you can convince yourself it partitions in a way that can be used to implement quicksort. Note that the pivot isn’t even ending up at the returned index! As a bonus, you can probably find a bug in my pseudocode above. This version is notoriously tricky! Here’s a quote from Jon Bentley’s Programming Pearls: “Most discussions of Quicksort use a partitioning scheme based on two approaching indices… (i.e. Hoare’s). Although the basic idea of that scheme is straightforward, I have always found the details tricky - I once spent the better part of two days chasing down a bug hiding in a short partitioning loop. A reader of a preliminary draft complained that the standard two-index method is in fact simpler than Lomuto’s and sketched some code to make his point; I stopped looking after I found two bugs.” Submission Submit an artifact of your work from 1 and 2."
  },"/pages/ActivitySort/": {
    "title": "Activity: Big-O Practice &amp; Sorting 1",
    "keywords": "Activity Sorting Big-O",
    "url": "/pages/ActivitySort/",
    "body": "Learning Goals You will work towards being able to… Prove growth functions are Big-$O, \\Omega, \\Theta$ from definition. Formalize algorithmic ideas into pseudocode Compute growth functions from time &amp; space complexities under the RAM model Know common algorithms to solve cannonical problems (sorting) Warm-Up Using our formal definition of Big-$O$… Show that $f(n) = \\log n \\in O(n)$. Note: When the inequality is obvious is a matter of contentious debate (depends on the audience!), but interpreting logarithms is strange but interpreting exponents is easier. If there is something that looks provable via induction like in Fleck, you can stop for now! Show that $g(n) = n + \\log n \\in O(n)$. Hint: Can the previous problem help you? Now let’s get a big more abstract: Consider non-negative functions $f, g$ such that $f(n) \\in O(g(n))$. Show that $f(n) + g(n) \\in O(g(n))$. Hint: Use the definition of Big-O to give you more information about the relationship between $f$ and $g$! Activity Instructions You will begin this activity with a set of 5 playing cards. Ignoring suit, your goal is to organize the cards using a standard ordering: $2 \\leq \\dots \\leq 9 \\leq 10 \\leq J \\leq Q \\leq K \\leq A$ (i.e., Ace high). The idea that underlies this activity is that you already know how to write a sort cards: If you have cards in front of you, you can confidently rearrange them into sorted order by implicitly comparing and swapping the positions of cards. What we will practice is formalizing the algorithm you use to do so by asking you to turn your intuitive sorting schemes into rigorous and well-defined algorithms based on the basic operations of comparison (is this card higher value than this other card?) and swapping (switch the position of this card with this other card). Look at the set of cards in front of you. Have a volunteer at your table slowly and methodically demonstrate how they’d rearrange the cards so that they are in increasing order using a sequence of comparisons between the values on cards and swaps. Try and document any local variables you need to update and keep track of as you go along. If you’re the volunteer, do this in a way that seems natural to you (and perhaps not in an unintuitive way that might be slightly more efficient for a computer!). Formalize the mechanics of your table’s chosen sorting technique. Describe it as a combination of a few basic operations — comparisons, and swapping the positions of two cards — along with standard operations like declaring and updating variables. Write out pseudocode to describe your sorting operation, doing your best to follow the format of the examples you’ve seen. Shuffle the cards and count the number of basic operations (swaps and comparisons) it takes to sort the array using your sorting algorithm. Now, consider how far from a best or worst case scenario the array you were given was. Try and see if you can can determine the worst-case input (of length 5!) for your sorting algorithm. If you think you’ve found the worst possible case, share your reasoning and try and convince your table-mates that no input would take any longer. How many time steps does your algorithm take in the worst case? Hint: Since all sorting cares about the relative ordering of elements, you only need to consider different initial shuffles of the 5 elements. Analyze the pseudocode for your algorithm and determine the growth function for your sort in terms of the number of comparisons and swaps. Determine it’s time complexity in terms of Big-$\\Theta$. If you have time, prove your growth function is, in fact, within that Big-$\\Theta$ time complexity class. Submit an artifact (picture, screenshot, etc.) of (1) your sorting pseudocode and (2) the corresponding worst-case growth function and submit it on Moodle."
  },"/pages/GraphApplications/": {
    "title": "Graph Applications",
    "keywords": "Activity Graph",
    "url": "/pages/GraphApplications/",
    "body": "Learning Goals You will work towards being able to… Translate problem specifications into graph problems to be solved with standard algorithms. Instructions Work with your groups on the following problems. You may work on them in any order: Problem 1: Two-Tabled Wedding (Skiena 7-15): You are organizing seating for a wedding where all guests in a list $V$ must be organized into two tables. You also have a list $E$ of pairs of people who hate each other. Discuss how you might, if possible, construct a table assignment that avoids any enemies being at the same table. What kind of a graph problem is this? Problem 2: Covering for a Friend (Skiena 7-18) A Vertex Cover for a graph $G = (V, E)$ is a set $V’ \\subset V$ (a subset of $V$ such that ever edge is incident to some $v \\in V’$ (formally, $\\forall (v_1, v_2) \\in E$, either $v_1 \\in V’$ or $v_2 \\in V’$). Informally, this just means for every edge, at least one vertex on the end of that edge is in $V’$. Suppose you have $\\lvert V \\rvert$ people working on $\\lvert E \\rvert$ projects in teams of 2. You want to organize a meeting where at least one person on each project is present to talk about progress on that project. This is a vertex cover problem — identify the vertices and edges of the implicit graph and convince yourself the solution is a vertex cover! Skiena 7-18 suggests one algorithm for finding a vertex cover is building a DFS tree and pruning the leaves. Construct an argument, given what you know about edges in a DFS tree, that this is always a vertex cover. Note that by Skiena’s definition, we disallow self-edges in our graphs! Also note that this is not required to be a minimum vertex cover, so don’t worry about optimality! Problem 3: Maximizing Utility Suppose you are a utility company contracted by a local government to connect a collection of houses to the utility grid. You’ve surveyed the site and determined the cost to connect each pair of buildings, including one that links up to the main grid. You would like to maximize the cost of connecting buildings (since you take a percentage of that cost as profit), but your contract specifies that you won’t be paid for redundant connections (connecting two buildings that are otherwises connected through another path in the grid). Find an efficient algorithm to compute the set of building-to-building connects that will maximize the cost to the local government. Problem 4: A Stop Along the Way (Skiena 8-19) Suppose you have a set of cities and edges representing roads connecting them. Edges are weighted with the travel time between the relevant cities. Determine an algorithm to find the shortest path between city $s$ and city $t$ that makes a stop at city $v$ along the way! Problem 5: Height Limited Shortest Paths (Skiena 8-22) Suppose you have a set of cities and of roads connecting pairs of cities. For each road, you are told the maximum allowable height of a vehicle navigating that road. Construct an algorithm to find the maximum height of a vehicle that could successfully transit from city $s$ to city $t$. Submission Submit an artifact of your work for one of the two problems."
  },"/pages/LoopInvariants/": {
    "title": "Loop Invariants and Asserts in Code",
    "keywords": "Supplement Loop Invariant Assert",
    "url": "/pages/LoopInvariants/",
    "body": "When learning about loop invariants, it may be helpful to see how they correspond to their equivalent assert-ions in code. That’s what this document aims to do, in Python (the closest thing to pseudocode one can get!). import numpy as np # This gives us real arrays in Python First, let’s implement insertion sort as close to our pseudocode as possible: def insertionSort(arr : np.array) -&gt; None: for i in range(1, len(arr)): j : int = i while (j &gt; 0) and (arr[j] &lt; arr[j-1]): # Swap! temp = arr[j] arr[j] = arr[j-1] arr[j-1] = temp j -= 1 Now let’s spot check that the algorithm works on some input: A = np.array([5,4,3,2,1]) print(\"Before:\\t\\t\", A) insertionSort(A) print(\"After:\\t\\t\", A) Before: [5 4 3 2 1] After: [1 2 3 4 5] Good! Now, our goal is to determine the reality of our loop invariant, so we must inspect the behavior of the algorithm on every loop iteration. We ask ourselves: How does this algorithm make progress in each iteration of the loop? Additionally, How can I formally declare that progress is being made? Let’s add a print statement to track the state of the array after each iteration of the outer for-loop to take a look: def insertionSort(arr : np.array) -&gt; None: for i in range(1, len(arr)): j : int = i while (j &gt; 0) and (arr[j] &lt; arr[j-1]): # Swap! temp = arr[j] arr[j] = arr[j-1] arr[j-1] = temp j -= 1 print(\"At i = {},\\t {}\".format(i, arr)) A = np.array([5,4,3,2,1]) # Worst-case input! print(\"Before:\\t\\t\", A) insertionSort(A) print(\"After:\\t\\t\", A) Before: [5 4 3 2 1] At i = 1, [4 5 3 2 1] At i = 2, [3 4 5 2 1] At i = 3, [2 3 4 5 1] At i = 4, [1 2 3 4 5] After: [1 2 3 4 5] Now, inspect the output and… LI 1: Construct a statement $P(n)$ that is true after the iteration where $i=n$ That is an invariant that is true after every loop iteration: A Loop Invariant!. Here, we note that we’re building a sorted subarray from the left edge that grows with each step: *After iteration $i$, arr[i] is sorted. Now to show that this loop invariant is true, we have to understand that insertion operation happening in each iteration. It’s helpful to decompose our insertion sort into two pieces: The part the performs the insert and the loop that repeatedly inserts. Let’s rewrite with this decomposition and more print statements: def insert(arr : np.array, i : int): \"\"\" inserts arr[i] into it's sorted position in arr[:i+1] Precondition: arr[:i] is sorted Postcondition: arr[:i+1] is sorted \"\"\" j : int = i # preserve the value of i while (j &gt; 0) and (arr[j] &lt; arr[j-1]): # Swap! temp = arr[j] arr[j] = arr[j-1] arr[j-1] = temp j -= 1 print(\"\\t After j = {}, \\t arr[:i+1] = \\t{}\" .format(j, arr[:i+1])) def insertionSort(arr : np.array) -&gt; None: for i in range(1, len(arr)): print(\"\\ni = {}\".format(i)) insert(arr, i) print(\"\\nAfter i = {},\\t\\t arr = \\t\\t{}\" .format(i, arr)) A = np.array([5,4,3,2,1]) # Worst-case input! print(\"Before:\\t\\t\\t\\t\\t{}\".format(A)) insertionSort(A) print(\"After:\\t\\t\\t\\t\\t{}\".format(A)) Before: [5 4 3 2 1] i = 1 After j = 0, arr[:i+1] = [4 5] After i = 1, arr = [4 5 3 2 1] i = 2 After j = 1, arr[:i+1] = [4 3 5] After j = 0, arr[:i+1] = [3 4 5] After i = 2, arr = [3 4 5 2 1] i = 3 After j = 2, arr[:i+1] = [3 4 2 5] After j = 1, arr[:i+1] = [3 2 4 5] After j = 0, arr[:i+1] = [2 3 4 5] After i = 3, arr = [2 3 4 5 1] i = 4 After j = 3, arr[:i+1] = [2 3 4 1 5] After j = 2, arr[:i+1] = [2 3 1 4 5] After j = 1, arr[:i+1] = [2 1 3 4 5] After j = 0, arr[:i+1] = [1 2 3 4 5] After i = 4, arr = [1 2 3 4 5] After: [1 2 3 4 5] LI 2: Construct a statement $P(n)$ that is true after the iteration where $j=n$ (for a fixed $i$) Create another loop invariant for a call to insert! After the iterations where $j=n$, we have that arr[j:i+1] is sorted! But this isn’t all (as we’ve seen in the proof of correctness): we know that arr[1:j] is also sorted (it began sorted and hasn’t been touched!), and we know that all elements in arr[1:j] are less than elements in arr[j+1:i+1] (these are the elements of the initially sorted subarray we moved to the right of the element we’re inserting!). All we’ve done is inspect a couple of outputs of the algorithm to make a claim about the code (namely, we claim that our two loop invariants hold!). This is identical to the process to arrive at the claims we make about the pseudocode, except instead of working through examples by hand, we’re running through real code! One fun thing that we can do in code is enforce our loop invariants through programming constructs like assert, which will throw an error if it’s argument evaluates to false. Here is one way of encoding our loop invariant into code: def isSorted(arr : np.array, i : int, j : int): \"\"\" Returns True if arr[i:j+1] (i.e., A[i...j]) is sorted. \"\"\" for k in range(i, j-1): if arr[k] &gt; arr[k+1]: return False return True def arrSmaller(arr1 : np.array, arr2 : np.array): for e in arr1: for f in arr2: if f &lt; e: return False return True def insert(arr : np.array, i : int): \"\"\" inserts arr[i] into it's sorted position in arr[:i+1] Precondition: arr[:i] is sorted Postcondition: arr[:i+1] is sorted \"\"\" j : int = i # preserve the value of i while (j &gt; 0) and (arr[j] &lt; arr[j-1]): # Swap! temp = arr[j] arr[j] = arr[j-1] arr[j-1] = temp j -= 1 assert (isSorted(arr, j, i+1) and isSorted(arr, 1, j) and arrSmaller(arr[:j],arr[j+1:i+1])), \"Insert LI False\" def insertionSort(arr : np.array) -&gt; None: for i in range(1, len(arr)): insert(arr, i) assert isSorted(arr, 0, i+1), \"insertionSort LI False\" And now we can show that these invariants are true for our sample (worst-case) input of length 5. A = np.array([5,4,3,2,1]) # Worst-case input! print(\"Before:\\t\\t\", A) insertionSort(A) print(\"After:\\t\\t\", A) Before: [5 4 3 2 1] After: [1 2 3 4 5] However, we can’t show that our loop invariant holds true for all inputs (for the same reason that testing can’t show that our algorithm is correct on all inputs through testing). We can, however, convince ourselves that it holds true for a random sample of inputs: Here we have our 2 LI asserts + a correctness assert going for 10 random arrays for each array size from 2 to 100: We get no assert failures! for _ in range(10): for n in range(2, 50): A = np.random.rand(n) insertionSort(A) assert (isSorted(A, 0, len(A))), \"Sorting Failed\" But we should convince ourselves that assert does actually do something! A = np.array([5,4,3,2,1]) assert isSorted(A,0, 4), \"This error message should appear\" --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) Cell In[48], line 2 1 A = np.array([5,4,3,2,1]) ----&gt; 2 assert isSorted(A,0, 4), \"This error message should appear\" AssertionError: This error message should appear And it does! The error message appears!"
  }}
